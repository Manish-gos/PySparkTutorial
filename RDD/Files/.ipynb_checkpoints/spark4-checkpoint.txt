Spark Transformations are the process of converting or transforming the given data RDD, which is immutable in nature into an another data RDD by applying some transformation logic to it. If you need some more details on What is Spark RDD? follow the link to learn in detail about Spark RDD. Most important point to be noted down is that when we apply transformation on top of any RDD in Spark, the operation is not performed immediately. It will save the list of operations to be performed on top of source RDD in a sequence by creating a DAG, (Directed Acyclic Graph). Once the Spark Action is called out, all the Transformation in the sequence of DAG will be executed. This property of Spark is defined as a Spark Lazy Execution. We can learn more in detail about the Transformation, Action and Speculative Execution in our upcoming chapters. Now to move on, Transformations are the basic operation executed on top of Spark RDD and few examples of transformations are Map, Flatmap, Filter, Mappartition, etc.
Let us consider a input file as a text file and it contains some sentence in it as shown below. Our task is to apply both map and flat map transformation one by one and observe the results produced to understand the working and gain knowledge on where to use Map and Flatmap. We do this by applying split() function on top Map() and FlatMap() in PySpark. Same logic can be applied in Scala and Java programming as well with slight modification to syntax.